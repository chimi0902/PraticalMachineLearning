---
title: "Practical Machine Learning: Course Project"
output: 
html_document:
md_document:
keep_md: false   
---

## Executive Summary

This document prensent the report of final assignment of Practical Machine Learning, wich is the 8th course on Specialization in Data science.

The aim here is predict the manner in which they did the exercise. For this we bouilt the model, used the cross validation to choice the best model. 

In this study, we only test two model: descision tree and Random forest. 

## Data

Given both training and test data from the following study:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
  
The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv 
  
## Package used 
```{r}
library(caret)
```

## Loading the data and Exploratory Analysis
```{r}
TrainFile <- "pml-training.csv"
if (!file.exists(TrainFile)) {
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    TrainFile <- "pml_training.csv"
    download.file(url, destfile = TrainFile)
}
TrainData <- read.csv(TrainFile, na.strings = c("NA","#DIV/0!",""))

TestFile <- "pml-testing.csv"
if (!file.exists(TestFile)) {
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url, destfile = TestFile)
}
TestData <- read.csv(TestFile, na.strings = c("NA","#DIV/0!",""))
```

## Cleaning Data
```{r}
# remove variables with Nearly Zero Variance
NZV <- nearZeroVar(TrainData)
TrainData <- TrainData[, -NZV]

# remove NA columun in train and test data
TrainData<-TrainData[,colSums(is.na(TrainData)) == 0]

# remove identification only variables (columns 1 to 5)

TrainData <- TrainData[, -(1:5)]

# Overview of train data using for calculation
#str(TrainData)
# Overview of test data using for calculation
#str(TestData)
```

## Find the best model 
# Partitioning train data 
```{r}
# create a partition with the training dataset 
inTrain  <- createDataPartition(TrainData$classe, p=0.7, list=FALSE)
SubTrain <- TrainData[inTrain, ]
SubTest  <- TrainData[-inTrain, ]
```

# Prediction used Decision Tree

```{r}
set.seed(135)

DecisionTreeModel2 <- train(classe ~ .,preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=SubTrain, method="rpart")

DecisionTreeModel2$finalModel

# prediction on Test dataset
DescisionTreepredict2 <- predict(DecisionTreeModel2, newdata=SubTest)
confMatDT2 <- confusionMatrix(DescisionTreepredict2, SubTest$classe)
print(confMatDT2,digit = 3)
```

# Prediction used Random Forest

```{r}
# model fit
set.seed(135)
controlRF <- trainControl(method="cv", number=3,verboseIter=FALSE)
RandForestModel2 <- train(classe ~ .,preProcess=c("center", "scale"), data=SubTrain, method="rf",trControl=controlRF)
RandForestModel2$finalModel

# prediction on Test dataset
RandForestpredict2 <- predict(RandForestModel2, newdata=SubTest)
confMatRF2 <- confusionMatrix(RandForestpredict2, SubTest$classe)
print(confMatRF2,digit = 3)
```
## Prediction on test data

Finally, I tested two model: descision Tree and random forest. this with and without normalization 

Summary of accuracy:

1- Descision Tree with normalzation: accuracy =  0.579

2- Random forest Tree with normalzation: accuracy = 0.998

To make a prediction on test data, I used the best fit model. This mean the model witch give the higher accuracy: Random forest with normalization.

```{r}
predictResult <- predict(RandForestModel2, newdata=TestData)
print(predictResult)
```

